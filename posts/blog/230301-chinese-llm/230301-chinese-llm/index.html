<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>中文 LLM 之路 | zhiqizhiqi</title><meta name=keywords content="LLM"><meta name=description content="在全球以英文为主的 LLM 研发和应用浪潮中，中文社区的 LLM 之路在哪"><meta name=author content="zhiqizhiqi"><link rel=canonical href=http://zhiqizhiqi.github.io/posts/blog/230301-chinese-llm/230301-chinese-llm/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=http://zhiqizhiqi.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://zhiqizhiqi.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://zhiqizhiqi.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://zhiqizhiqi.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://zhiqizhiqi.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="中文 LLM 之路"><meta property="og:description" content="在全球以英文为主的 LLM 研发和应用浪潮中，中文社区的 LLM 之路在哪"><meta property="og:type" content="article"><meta property="og:url" content="http://zhiqizhiqi.github.io/posts/blog/230301-chinese-llm/230301-chinese-llm/"><meta property="og:image" content="http://zhiqizhiqi.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-02-28T11:30:03+00:00"><meta property="article:modified_time" content="2023-02-28T11:30:03+00:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://zhiqizhiqi.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="中文 LLM 之路"><meta name=twitter:description content="在全球以英文为主的 LLM 研发和应用浪潮中，中文社区的 LLM 之路在哪"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"","item":"http://zhiqizhiqi.github.io/posts/"},{"@type":"ListItem","position":2,"name":"","item":"http://zhiqizhiqi.github.io/posts/blog/"},{"@type":"ListItem","position":3,"name":"中文 LLM 之路","item":"http://zhiqizhiqi.github.io/posts/blog/230301-chinese-llm/230301-chinese-llm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"中文 LLM 之路","name":"中文 LLM 之路","description":"在全球以英文为主的 LLM 研发和应用浪潮中，中文社区的 LLM 之路在哪","keywords":["LLM"],"articleBody":"本文围绕“中文 LLM 之路”展开一系列思考和论述。\n需求：中文社区需要一个专用的 LLM 模型么？ 技术：怎么做？哪些大模块？哪些挑战？如何应对？Milestone 是什么？ 资源：需要多少钱？需要多少人？需要多少时间？需要多少卡？ 竞争：面对大厂？面对创业公司？全球化？ 商业：长期目标是什么？短期目标是什么？路径是什么？和技术研发阶段的关系是什么？ 研究：通往 AGI 之路？ Research 创业？技术创业？商业模式创业？\n1. 需求 考虑到这个问题的复杂程度，如下的论述大概率是有局限性的，我会持续不断的完善这部分的信息收集和思考。\n从过去的历史来看，至少中国市场和政治经济两大要素影响了核心产业在中国是否存在独立的可能性\n中国市场规模大，这种规模效应容易产生针对中国市场具体情况定制的产品和服务，例如本地生活在中美两地的差异。另外一方面，中国的人口结构和收入结构也有特殊性，AI 改善生产效率的商业价值和发达国家不同 中美竞争大背景下的政治经济要素在最近 3~5 年更是主导了若干产业在中国的发展，纯粹的经济模型对产业链格局演化逐步被科技含金量所代替，越是前沿的科技越是要“自主可控” 在宏观经济学没有能力全面论述的情况下，LLM 以人工智能最前沿的定性似乎也基本决定了其在中国市场“自主可控”的必要性。当然，对于实际的商业路径上，也必须围绕中国市场的特点。\n2. 技术 对于中文 LLM 的技术之路，大致可以分成如下三个大的步骤，如果是从零开始大概 18 个月：\nstep-1-复现 SOTA：重点是将算法和训练系统推进到 SOTA 水平。目前以英文为主的 LLM 工作层出不穷，并且有非常全面的详细信息来确认，难点在于高效训练系统的搭建以及 large scale training debug 方法论\u0026技巧。 step-2-中文 LLM：在 SOTA 的算法和训练系统上，重点解决数据和 Evaluation 系统两大模块的问题。不幸的是，这两个环节对于中文来说，开源社区的成果并不丰富。对于数据来说，中文高质量数据天生短缺如何解决，已有数字化内容中如何筛选？对于 Evaluation 系统来说更是难题，相对于层出不穷的英文 NLP Downstream Tasks and Dataset，中文的公开测试集有些捉襟见肘（不仅仅是多样性不够丰富，而且距离最前沿不够近），例如 CUGE / CLUE / MUGE，这些够么？ step-3-DemoApp：在有足够好的 LLM 后，还需要解决产品定义、应用算法 finetune 和 (option) 推理效率问题，以推出让用户直接可感知的 Demo 应用。对于产品定义来说，在中文社区内，最好的 demo 形式是 chat 么？应用算法 finetune 过程中，势必需要将标注体系构建起来（偷懒调用 chatgpt 接口或许是“饮鸠止渴”）。最后，如果不小心 DemoApp 过于火爆，推理效率的问题也是不得不提上日常的事项。 如上三个步骤，也仅仅是中文 LLM 技术之路的 “A 轮阶段” ，可预见的未来至少还包括：\nB 轮阶段：技术继续推动的同时进行产品化 继续推动技术，这里的方向很多，后面专门阐述 提升产品体验直至 PMF ，至少要解决推理成本在产品财务模型上可接受、特定场景算法性能可用和好用的问题； C 轮阶段：商业化 Scale ；上云，例如 PaaS 层 PMF ； 本 section 暂时重点讨论 A 轮阶段 的内容，上文阐述的三个步骤中每个步骤研发的重点模块都不相同，这也直接影响了资源引入的节奏\nstep 算法 训练系统 训练数据 Evaluation 系统 应用(Alignment) 推理效率 step-1-复现 SOTA √ √ x x x x step-2-中文 LLM √ √ √ √ x x step-3-DemoApp √ √ √ √ √ √ 2.1 step-1-复现 SOTA 对于复现 SOTA 来说，主要的挑战是算法和训练系统，次要挑战是训练数据模块。虽然也涉及 Eval 系统的工作，这部分更接近确定性的软件工程工作（设计框架可以 scalable 集成更多的测试集），所以懂机器学习训练的不错的软件架构师即可做好。\n算法是公开资料披露最为详细的部分，代码和论文中有非常详细的阐述，按理来说是非常容易复现的。但是 model debug 的难度随着 scale 上升会极具上升，例如在训练周期长达 60 天的 256 卡的训练任务到第 30 天时 loss 开始随机出现 NaN ，这个问题该如何高效的着手？任何的改动都触发从头训练进行验证么？如果不是，从最近一个“正常”的 checkpoint 开始 apply 调整是否可行（如何验证某个 checkpoint 是正常 的呢）？任何 large scale 上出现的异常状态几乎都面临尝试成本极高的问题。某种视角下，这部分的经验或 knowhow 可能是算法中极其重要但又不是广为熟知的要素了。\n训练系统从整体来看，有硬件和软件两个部分。\n硬件部分包含 GPU 选型、服务器配置、机房网络设置。这里需要在配置硬件时就对训练时的计算、访存、传输之间的关系进行分析，不要出现 8*A100 的豪华 GPU 组合配置一个 10G 网卡的尴尬情况。虽然今天 LLM 训练似乎 80GB A100 已经是个标配，但芯片创业公司的影响力不容忽视，谁知道哪天 Jim Keller 的 Tenstorrent 在训练芯片上丢出个吊打 NV 的东西。 软件部分包含的内容很多，这里开源项目也很多，例如算子优化中 xFormers 中提供若干高效的 fused cuda 实现（其他如 flash-attention)，并行中 ColossalAI 给出的 Tensor Parallel 以及计算传输的调度。需要动态看待开源社区的发展（层出不穷的小模块），在架构上做出非常好的抽象，使得可以源源不断的从开源社区获益（在大量历史功能的训练代码上往往无法高效集成新的开源模块，甚至是无法做到）。 此步骤中，核心的 metric 是 power law 曲线的趋势能够对齐，如果资金充裕，那么额外花钱训练100B 量级参数的 SOTA 模型是个更加保险的举措（虽然经济上很不值）。然而，随着开源社区日益活跃，算法和训练系统两大模块中会有越来越多的子集可以“免费”获得。做极端一些的设想，假设某个团队直接将训练代码和模型全部开源，那么是否意味着中文 LLM 的门槛就显著下降了呢？目前来看，很有可能不是这样的，这也是 step-2 需要攻坚的部分。\n2.2 step-2-中文 LLM 当以英文为主的 SOTA LLM 模型向中文迁移时，一方面训练数据需要进行调整，公开数据中虽然是 multi-lingual，但这对中文来说远远不足；另一方面是 Evaluation 系统的适配，度量中文 LLM 在具体任务上的算法性能是否是现有几个公开的中文 NLP Task or Dataset 就足够的呢？\n中文训练数据中最常见的问题是中文高质量数据远远少于英文的问题如何解决，甚至部分学术领域几乎只有英文数据（即零和一的差异）。我们是期待模型能够从英文数据中泛化到中文么（即在 loss 或训练数据设计上进行改进）？还是考虑利用翻译模块来进行辅助（如果是的，那么是训练数据中将全部英文翻译为中文，还是推理时将中文翻译为英文）？甚至是暂时考虑放弃解决这个问题（接受高质量内容就是用英文表达的社会性规律）？这个问题思考和实践的深入程度或许是中文 LLM 研发质量的重要标志之一。\n另外，如何度量数据的质量也是个需要想清楚的问题。在有完备评测系统时，至少在不考虑代价时可以通过 ablation study 来评价，而糟糕的事情在于对于中文 LLM 来说评测能力可能还差距很远，那么此时研发人员主观的评价显得更加重要，这里可以引出两个值得探讨的问题\n如何设计 heuristic 的策略来自动化的找出高质量的数据？英文高质量数据的特征是否帮助自动化找出中文高质量的数据？LM 本身 inference 时的 entropy 是否有明显的区分效果？ 如何主观标注数据的质量？哪些维度需要被考虑，informative? truthfullness? … 回到数据量，另一个思路是，中文社区高度发达的短视频是否可以作为文本数据的重要补充呢？Openai Whisper 是否也是用来撬动英文大量播客、视频数据的杠杆呢？\nEvaluation 系统\n2.3 step-3-DemoApp 2.4 更长期的技术体系（2~3 年） 上述内容仍然只是着眼于 18 个月的周期的事情，而星辰大海之路又怎会止步于此呢。当着眼于更长期时，技术演进的方向是什么呢？技术迭代的速度如何提升呢？\n2.4.1 现有 LLM 技术方向 信息或知识的可控性（Alignment）的能力 RLHF Alignment 演进 模型 explicit memory 机制，实现信息的实时性、可控性 更低的训练和推理成本 模型 continue learning 的能力，如何利用已经训练好的 Large Model，而不是每次 from scratch 推理成本的下降 更强的算法性能和小样本学习能力 Inductive Bias 优化 Proxy Task：更合适（比 LM）的挖掘数据内生结构的任务是什么？ Prompt：如何 task driven 的找出最合适的 Prompt？CoT 怎么更 systematic 优化？ 模型结构：transformer 上更多小改动的优化？transformer 之后的下一个“超级结构”是啥？ 更大规模：对人类文明的内容“learning”更广更深 模型：引入 MoE 机制，进一步 x100 倍规模到 10T 参数量级 数据：音频和视频中的文本数据？利用 LLM Inference（接近与无穷）的数据？利用逻辑体系的公理来自动化的生成数据？ 框架 生成器 with 判别器的模式：先验来自于评价好坏总是比生成容易； Self-Play（引发质变点）：如何让 LLM 帮助 LLM 提升（例如超大量 sampling inference 数据，进行打分找出高质量数据）？ 引入更多 API：补充 LLM 基础能力缺失（例如复杂数值计算），补充 report bias 等 Multi-Modal！ 2.4.2 现有 LLM 迭代效率 验证效率：LLM 极高的训练成本使得验证 idea 的有效性极其昂贵，此项成本的优化程度几乎约等于整个领域的进展速度 数据体系：有用户持续使用贡献数据以及算法团队处理的 Data Engine！ 2.4.3 更 Foundation 的要素 Scale！：不断冲击工程极限，榨干数据、算力的价值，Emergent Ability 随之而来。（这需要信仰？） 超高人才密度！ 3. 资源 资源上暂时只考虑钱、人、时间、卡*几个维度\n时间 钱 人 卡 目标 12 个月 $12M~$15m 30 100~300*A100 24 个月 36 个月 12 个月\n24 个月\n36 个月\n","wordCount":"389","inLanguage":"en","datePublished":"2023-02-28T11:30:03Z","dateModified":"2023-02-28T11:30:03Z","author":{"@type":"Person","name":"zhiqizhiqi"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://zhiqizhiqi.github.io/posts/blog/230301-chinese-llm/230301-chinese-llm/"},"publisher":{"@type":"Organization","name":"zhiqizhiqi","logo":{"@type":"ImageObject","url":"http://zhiqizhiqi.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://zhiqizhiqi.github.io/ accesskey=h title="Home (Alt + H)"><img src=http://zhiqizhiqi.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://zhiqizhiqi.github.io/archives/ title=archives><span>archives</span></a></li><li><a href=http://zhiqizhiqi.github.io/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://zhiqizhiqi.github.io/>Home</a>&nbsp;»&nbsp;<a href=http://zhiqizhiqi.github.io/posts/></a>&nbsp;»&nbsp;<a href=http://zhiqizhiqi.github.io/posts/blog/></a></div><h1 class=post-title>中文 LLM 之路<sup><span class=entry-isdraft>&nbsp;&nbsp;[draft]</span></sup></h1><div class=post-description>在全球以英文为主的 LLM 研发和应用浪潮中，中文社区的 LLM 之路在哪</div><div class=post-meta><span title='2023-02-28 11:30:03 +0000 UTC'>February 28, 2023</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;389 words&nbsp;·&nbsp;zhiqizhiqi&nbsp;|&nbsp;<a href=https://github.com/zhiqizhiqi/zhiqizhiqi.github.io/blob/main/content//posts/blog/230301-chinese-llm/230301-chinese-llm.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#1-需求>1. 需求</a></li><li><a href=#2-技术>2. 技术</a><ul><li><a href=#21-step-1-复现-sota>2.1 step-1-复现 SOTA</a></li><li><a href=#22-step-2-中文-llm>2.2 step-2-中文 LLM</a></li><li><a href=#23-step-3-demoapp>2.3 step-3-DemoApp</a></li><li><a href=#24-更长期的技术体系23-年>2.4 更长期的技术体系（2~3 年）</a></li></ul></li><li><a href=#3-资源>3. 资源</a></li></ul></nav></div></details></div><div class=post-content><p>本文围绕“中文 LLM 之路”展开一系列思考和论述。</p><ol><li>需求：中文社区需要一个专用的 LLM 模型么？</li><li>技术：怎么做？哪些大模块？哪些挑战？如何应对？Milestone 是什么？</li><li>资源：需要多少钱？需要多少人？需要多少时间？需要多少卡？</li><li>竞争：面对大厂？面对创业公司？全球化？</li><li>商业：长期目标是什么？短期目标是什么？路径是什么？和技术研发阶段的关系是什么？</li><li>研究：通往 AGI 之路？</li></ol><p>Research 创业？技术创业？商业模式创业？</p><h2 id=1-需求>1. 需求<a hidden class=anchor aria-hidden=true href=#1-需求>#</a></h2><p>考虑到这个问题的复杂程度，如下的论述大概率是有局限性的，我会持续不断的完善这部分的信息收集和思考。</p><p>从过去的历史来看，至少中国市场和政治经济两大要素影响了核心产业在中国是否存在独立的可能性</p><ul><li>中国市场规模大，这种规模效应容易产生针对中国市场具体情况定制的产品和服务，例如本地生活在中美两地的差异。另外一方面，中国的人口结构和收入结构也有特殊性，AI 改善生产效率的商业价值和发达国家不同</li><li>中美竞争大背景下的政治经济要素在最近 3~5 年更是主导了若干产业在中国的发展，纯粹的经济模型对产业链格局演化逐步被科技含金量所代替，越是前沿的科技越是要“自主可控”</li></ul><p>在宏观经济学没有能力全面论述的情况下，LLM 以人工智能最前沿的定性似乎也基本决定了其在中国市场“自主可控”的必要性。当然，对于实际的商业路径上，也必须围绕中国市场的特点。</p><h2 id=2-技术>2. 技术<a hidden class=anchor aria-hidden=true href=#2-技术>#</a></h2><p>对于中文 LLM 的技术之路，大致可以分成如下三个大的步骤，如果是从零开始大概 18 个月：</p><ul><li><strong>step-1-复现 SOTA</strong>：重点是将算法和训练系统推进到 SOTA 水平。目前以英文为主的 LLM 工作层出不穷，并且有非常全面的详细信息来确认，难点在于高效训练系统的搭建以及 large scale training debug 方法论&技巧。</li><li><strong>step-2-中文 LLM</strong>：在 SOTA 的算法和训练系统上，重点解决数据和 Evaluation 系统两大模块的问题。不幸的是，这两个环节对于中文来说，开源社区的成果并不丰富。对于数据来说，中文高质量数据天生短缺如何解决，已有数字化内容中如何筛选？对于 Evaluation 系统来说更是难题，相对于层出不穷的英文 NLP Downstream Tasks and Dataset，中文的公开测试集有些捉襟见肘（不仅仅是多样性不够丰富，而且距离最前沿不够近），例如 <a href=http://cuge.baai.ac.cn/pdf/CUGE.pdf>CUGE</a> / <a href=https://www.cluebenchmarks.com/>CLUE</a> / <a href=https://tianchi.aliyun.com/muge>MUGE</a>，这些够么？</li><li><strong>step-3-DemoApp</strong>：在有足够好的 LLM 后，还需要解决产品定义、应用算法 finetune 和 (option) 推理效率问题，以推出让用户直接可感知的 Demo 应用。对于产品定义来说，在中文社区内，最好的 demo 形式是 chat 么？应用算法 finetune 过程中，势必需要将标注体系构建起来（偷懒调用 chatgpt 接口或许是“饮鸠止渴”）。最后，如果不小心 DemoApp 过于火爆，推理效率的问题也是不得不提上日常的事项。</li></ul><p>如上三个步骤，也仅仅是中文 LLM 技术之路的 “<strong>A 轮阶段</strong>” ，可预见的未来至少还包括：</p><ul><li>B 轮阶段：技术继续推动的同时进行产品化<ul><li>继续推动技术，这里的方向很多，后面专门阐述</li><li>提升产品体验直至 PMF ，至少要解决推理成本在产品财务模型上可接受、特定场景算法性能可用和好用的问题；</li></ul></li><li>C 轮阶段：商业化 Scale ；上云，例如 PaaS 层 PMF ；</li></ul><p>本 section 暂时重点讨论 <strong>A 轮阶段</strong> 的内容，上文阐述的三个步骤中每个步骤研发的重点模块都不相同，这也直接影响了资源引入的节奏</p><table><thead><tr><th>step</th><th style=text-align:center>算法</th><th style=text-align:center>训练系统</th><th style=text-align:center>训练数据</th><th style=text-align:center>Evaluation 系统</th><th style=text-align:center>应用(Alignment)</th><th style=text-align:center>推理效率</th></tr></thead><tbody><tr><td>step-1-复现 SOTA</td><td style=text-align:center>√</td><td style=text-align:center>√</td><td style=text-align:center>x</td><td style=text-align:center>x</td><td style=text-align:center>x</td><td style=text-align:center>x</td></tr><tr><td>step-2-中文 LLM</td><td style=text-align:center>√</td><td style=text-align:center>√</td><td style=text-align:center>√</td><td style=text-align:center>√</td><td style=text-align:center>x</td><td style=text-align:center>x</td></tr><tr><td>step-3-DemoApp</td><td style=text-align:center>√</td><td style=text-align:center>√</td><td style=text-align:center>√</td><td style=text-align:center>√</td><td style=text-align:center>√</td><td style=text-align:center>√</td></tr></tbody></table><h3 id=21-step-1-复现-sota>2.1 step-1-复现 SOTA<a hidden class=anchor aria-hidden=true href=#21-step-1-复现-sota>#</a></h3><p>对于复现 SOTA 来说，主要的挑战是算法和训练系统，次要挑战是训练数据模块。虽然也涉及 Eval 系统的工作，这部分更接近确定性的软件工程工作（设计框架可以 scalable 集成更多的测试集），所以懂机器学习训练的不错的软件架构师即可做好。</p><p><strong>算法</strong>是公开资料披露最为详细的部分，代码和论文中有非常详细的阐述，按理来说是非常容易复现的。但是 <strong>model debug 的难度随着 scale 上升会极具上升</strong>，例如在训练周期长达 60 天的 256 卡的训练任务到第 30 天时 loss 开始随机出现 NaN ，这个问题该如何高效的着手？任何的改动都触发从头训练进行验证么？如果不是，从最近一个“<strong>正常</strong>”的 checkpoint 开始 apply 调整是否可行（如何验证某个 checkpoint 是<strong>正常</strong> 的呢）？任何 large scale 上出现的异常状态几乎都面临尝试成本极高的问题。某种视角下，这部分的经验或 knowhow 可能是算法中极其重要但又不是广为熟知的要素了。</p><p><strong>训练系统</strong>从整体来看，有硬件和软件两个部分。</p><ul><li>硬件部分包含 GPU 选型、服务器配置、机房网络设置。这里需要在配置硬件时就对训练时的计算、访存、传输之间的关系进行分析，不要出现 8*A100 的豪华 GPU 组合配置一个 10G 网卡的尴尬情况。虽然今天 LLM 训练似乎 80GB A100 已经是个标配，但芯片创业公司的影响力不容忽视，谁知道哪天 Jim Keller 的 Tenstorrent 在训练芯片上丢出个吊打 NV 的东西。</li><li>软件部分包含的内容很多，这里开源项目也很多，例如算子优化中 <a href=https://github.com/facebookresearch/xformers>xFormers</a> 中提供若干高效的 fused cuda 实现（其他如 <a href=https://github.com/HazyResearch/flash-attention>flash-attention</a>)，并行中 <a href=https://github.com/hpcaitech/ColossalAI>ColossalAI</a> 给出的 Tensor Parallel 以及计算传输的调度。需要动态看待开源社区的发展（层出不穷的小模块），在架构上做出非常好的抽象，使得可以源源不断的从开源社区获益（在大量历史功能的训练代码上往往无法高效集成新的开源模块，甚至是无法做到）。</li></ul><p>此步骤中，核心的 metric 是 power law 曲线的趋势能够对齐，如果资金充裕，那么额外花钱训练100B 量级参数的 SOTA 模型是个更加保险的举措（虽然经济上很不值）。然而，随着开源社区日益活跃，算法和训练系统两大模块中会有越来越多的子集可以“<strong>免费</strong>”获得。做极端一些的设想，假设某个团队直接将训练代码和模型全部开源，那么是否意味着中文 LLM 的门槛就显著下降了呢？目前来看，很有可能不是这样的，这也是 step-2 需要攻坚的部分。</p><h3 id=22-step-2-中文-llm>2.2 step-2-中文 LLM<a hidden class=anchor aria-hidden=true href=#22-step-2-中文-llm>#</a></h3><p>当以英文为主的 SOTA LLM 模型向中文迁移时，一方面训练数据需要进行调整，公开数据中虽然是 multi-lingual，但这对中文来说远远不足；另一方面是 Evaluation 系统的适配，度量中文 LLM 在具体任务上的算法性能是否是现有几个公开的中文 NLP Task or Dataset 就足够的呢？</p><p><strong>中文训练数据</strong>中最常见的问题是中文高质量数据远远少于英文的问题如何解决，甚至部分学术领域几乎只有英文数据（即零和一的差异）。我们是期待模型能够从英文数据中泛化到中文么（即在 loss 或训练数据设计上进行改进）？还是考虑利用翻译模块来进行辅助（如果是的，那么是训练数据中将全部英文翻译为中文，还是推理时将中文翻译为英文）？甚至是暂时考虑放弃解决这个问题（接受高质量内容就是用英文表达的社会性规律）？这个问题思考和实践的深入程度或许是中文 LLM 研发质量的重要标志之一。</p><p>另外，如何度量数据的质量也是个需要想清楚的问题。在有完备评测系统时，至少在不考虑代价时可以通过 ablation study 来评价，而糟糕的事情在于对于中文 LLM 来说评测能力可能还差距很远，那么此时研发人员主观的评价显得更加重要，这里可以引出两个值得探讨的问题</p><ul><li>如何设计 heuristic 的策略来自动化的找出高质量的数据？英文高质量数据的特征是否帮助自动化找出中文高质量的数据？LM 本身 inference 时的 entropy 是否有明显的区分效果？</li><li>如何主观标注数据的质量？哪些维度需要被考虑，informative? truthfullness? &mldr;</li></ul><p>回到数据量，另一个思路是，中文社区高度发达的短视频是否可以作为文本数据的重要补充呢？Openai Whisper 是否也是用来撬动英文大量播客、视频数据的杠杆呢？</p><p><strong>Evaluation 系统</strong></p><h3 id=23-step-3-demoapp>2.3 step-3-DemoApp<a hidden class=anchor aria-hidden=true href=#23-step-3-demoapp>#</a></h3><h3 id=24-更长期的技术体系23-年>2.4 更长期的技术体系（2~3 年）<a hidden class=anchor aria-hidden=true href=#24-更长期的技术体系23-年>#</a></h3><p>上述内容仍然只是着眼于 18 个月的周期的事情，而星辰大海之路又怎会止步于此呢。当着眼于更长期时，技术演进的方向是什么呢？技术迭代的速度如何提升呢？</p><h4 id=241-现有-llm-技术方向>2.4.1 现有 LLM 技术方向<a hidden class=anchor aria-hidden=true href=#241-现有-llm-技术方向>#</a></h4><ul><li>信息或知识的可控性（Alignment）的能力<ul><li>RLHF Alignment 演进</li><li>模型 explicit memory 机制，实现信息的实时性、可控性</li></ul></li><li>更低的训练和推理成本<ul><li>模型 continue learning 的能力，如何利用已经训练好的 Large Model，而不是每次 from scratch</li><li>推理成本的下降</li></ul></li><li>更强的算法性能和小样本学习能力<ul><li>Inductive Bias 优化<ul><li>Proxy Task：更合适（比 LM）的挖掘数据内生结构的任务是什么？</li><li>Prompt：如何 task driven 的找出最合适的 Prompt？CoT 怎么更 systematic 优化？</li><li>模型结构：transformer 上更多小改动的优化？transformer 之后的下一个“超级结构”是啥？</li></ul></li><li>更大规模：对人类文明的内容“learning”更广更深<ul><li>模型：引入 MoE 机制，进一步 x100 倍规模到 10T 参数量级</li><li>数据：音频和视频中的文本数据？利用 LLM Inference（接近与无穷）的数据？利用逻辑体系的公理来自动化的生成数据？</li></ul></li><li>框架<ul><li>生成器 with 判别器的模式：先验来自于评价好坏总是比生成容易；</li><li>Self-Play（引发质变点）：如何让 LLM 帮助 LLM 提升（例如超大量 sampling inference 数据，进行打分找出高质量数据）？</li><li>引入更多 API：补充 LLM 基础能力缺失（例如复杂数值计算），补充 report bias 等</li></ul></li></ul></li><li>Multi-Modal！</li></ul><h4 id=242-现有-llm-迭代效率>2.4.2 现有 LLM 迭代效率<a hidden class=anchor aria-hidden=true href=#242-现有-llm-迭代效率>#</a></h4><ul><li>验证效率：LLM 极高的训练成本使得验证 idea 的有效性极其昂贵，此项成本的优化程度几乎约等于整个领域的进展速度</li><li>数据体系：有用户持续使用贡献数据以及算法团队处理的 Data Engine！</li></ul><h4 id=243-更-foundation-的要素>2.4.3 更 Foundation 的要素<a hidden class=anchor aria-hidden=true href=#243-更-foundation-的要素>#</a></h4><ul><li>Scale！：不断冲击工程极限，榨干数据、算力的价值，Emergent Ability 随之而来。（这需要信仰？）</li><li>超高人才密度！</li></ul><h2 id=3-资源>3. 资源<a hidden class=anchor aria-hidden=true href=#3-资源>#</a></h2><p>资源上暂时只考虑钱、人、时间、卡*几个维度</p><table><thead><tr><th style=text-align:center>时间</th><th style=text-align:center>钱</th><th style=text-align:center>人</th><th style=text-align:center>卡</th><th style=text-align:left>目标</th></tr></thead><tbody><tr><td style=text-align:center>12 个月</td><td style=text-align:center>$12M~$15m</td><td style=text-align:center>30</td><td style=text-align:center>100~300*A100</td><td style=text-align:left></td></tr><tr><td style=text-align:center>24 个月</td><td></td><td></td><td></td><td></td></tr><tr><td style=text-align:center>36 个月</td><td></td><td></td><td></td><td></td></tr></tbody></table><p><strong>12 个月</strong></p><p><strong>24 个月</strong></p><p><strong>36 个月</strong></p></div><footer class=post-footer><ul class=post-tags><li><a href=http://zhiqizhiqi.github.io/tags/llm/>LLM</a></li></ul><nav class=paginav><a class=prev href=http://zhiqizhiqi.github.io/posts/newletters/weekly-230304/><span class=title>« Prev</span><br><span>Week-AI-01：LLaMA</span></a>
<a class=next href=http://zhiqizhiqi.github.io/posts/newletters/weekly-230224/><span class=title>Next »</span><br><span>Week-AI-00：ToolFormer; ControlNet</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share 中文 LLM 之路 on twitter" href="https://twitter.com/intent/tweet/?text=%e4%b8%ad%e6%96%87%20LLM%20%e4%b9%8b%e8%b7%af&amp;url=http%3a%2f%2fzhiqizhiqi.github.io%2fposts%2fblog%2f230301-chinese-llm%2f230301-chinese-llm%2f&amp;hashtags=LLM"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 中文 LLM 之路 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2fzhiqizhiqi.github.io%2fposts%2fblog%2f230301-chinese-llm%2f230301-chinese-llm%2f&amp;title=%e4%b8%ad%e6%96%87%20LLM%20%e4%b9%8b%e8%b7%af&amp;summary=%e4%b8%ad%e6%96%87%20LLM%20%e4%b9%8b%e8%b7%af&amp;source=http%3a%2f%2fzhiqizhiqi.github.io%2fposts%2fblog%2f230301-chinese-llm%2f230301-chinese-llm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 中文 LLM 之路 on reddit" href="https://reddit.com/submit?url=http%3a%2f%2fzhiqizhiqi.github.io%2fposts%2fblog%2f230301-chinese-llm%2f230301-chinese-llm%2f&title=%e4%b8%ad%e6%96%87%20LLM%20%e4%b9%8b%e8%b7%af"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 中文 LLM 之路 on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2fzhiqizhiqi.github.io%2fposts%2fblog%2f230301-chinese-llm%2f230301-chinese-llm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 中文 LLM 之路 on whatsapp" href="https://api.whatsapp.com/send?text=%e4%b8%ad%e6%96%87%20LLM%20%e4%b9%8b%e8%b7%af%20-%20http%3a%2f%2fzhiqizhiqi.github.io%2fposts%2fblog%2f230301-chinese-llm%2f230301-chinese-llm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 中文 LLM 之路 on telegram" href="https://telegram.me/share/url?text=%e4%b8%ad%e6%96%87%20LLM%20%e4%b9%8b%e8%b7%af&amp;url=http%3a%2f%2fzhiqizhiqi.github.io%2fposts%2fblog%2f230301-chinese-llm%2f230301-chinese-llm%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=http://zhiqizhiqi.github.io/>zhiqizhiqi</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>