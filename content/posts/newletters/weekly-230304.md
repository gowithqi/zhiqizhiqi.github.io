---
title: "Week-AI-01：LLaMA; ViT 22B; Robo LLM(PaLM-E = ViT22B + PaLM 560B); Image RLHF;"
date: 2023-03-11T15:30:03+00:00
# weight: 1
# aliases: ["/first"]
tags: ["Week"]
# author: ["Me", "You"] # multiple authors

draft: false

description: "通用物理世界机器人的雏形"
# canonicalURL: "https://canonical.url/to/page"
disableHLJS: true # to disable highlightjs
disableShare: false
disableHLJS: false
hideSummary: false
searchHidden: true
---

## 0. TakeAway & 思考
1. 图像生成领域的可控性要求逐步提升，这也是目前学术界和工业界的研究主线，但缺少一个统一的框架来建模所有对图片潜在的操作，这个框架的收敛将是整个技术栈收敛的关键，这使得各种可控性能够任意组合，满足所有潜在的需求（比如我希望根据 prompt 生成与某个特定图片风格一致，和另外一个图片 edge detection 一致的图片时就无法实现）
2. LLaMA 开源会加速 LLM 应用的研究和探索，期待大模型基础上可以玩出怎样的花样。同时，LLM 的开源生态在商业上会产生怎样的影响也是一个值得创业者思考的问题，“羊毛出在猪身上”，LLM 开源的“猪”是什么呢？
3. 图像领域的 proxy task 应该是什么？使用 JFT 训练是更好的选择么？
   1. 还比较值得考虑的是 Mulit-Modal 之间的互监督以及 Masked Autoencoder。前者来自于数据自然呈现的结果更加 scalable（而依赖 JFT 势必更加依赖离线 auto-label 流程以及可能的人工标注，都在可扩展性上不够好），后者能够捕捉数据内在的结构，对于表征学习来说也很重要。
   2. 从框架的可扩展性和要素成熟度来看，图片相对于文本明显还有更长的道路要走
4. 越来越明显地一个趋势是让机器先将人类世界现存的所有数据都先"learn"一遍，以此快速提升模型的通用性，而这个模型也将成为所有 AI 系统的必要组件。
5. 对多模态来说，ground to 物理世界是最重要的突破点，**将通用智能从虚拟世界带到物理世界**。而物理世界的智能体可能更加是“国家机器”关注的形态，也是必争之地。如果看三年后创投圈什么机会将再次出现，很有可能是通用物理机器人。

## 1. 前沿技术追踪
### 1.1 LLaMA：开源 LLM，撬动全球零散算力和脑力资源的杠杆
Meta 汇总 GPT-3 之后近两年所有重要的 LLM 改进，训练了 LLM 模型，规模从 7B 到 65B，在大量 NLP Task 的评测上大部分比 GPT-3 & Chinchilla-70B & PaLM-540B 更好，更重要的是此项工作完成基于公开数据，并且模型会进行开源分享！

**如何实现的？** 
1. 数据：除了广泛被使用的 CommonCrawl & C4 & Wikipedia & Book 之外，还直接使用了 Github & Arxiv & StackExchange，分别对应代码、（各个学科）科学论文和高质量的问答数据。注：这些改动为此项工作后续的实验结果解读增加了很多干扰
2. 网络结构：在原始 transformer 上引入了 pre-normalization（提升训练稳定性）、SwiGLU（提升性能）、Rotary Embeddings（position embedding 改进）。
3. 训练加速：针对 MHA（multi-head attention）实现了更高效的代码（注：xformers 内有对应实现）减少内存和计算；若干 common 操作：forward activation checkpoint（细节在于手动指定层，而不是自动计算的结果，更加快） & 并行 & 计算和传输并行等。
4. 训练效率：在 2048 块 A100 卡上训练 65B 模型，380 tokens/sec/GPU，21 天训练了 1.4T tokens。注：这个结果来看，并不是个昂贵的配置，在实践中，两个月的训练周期仍然在可接受的范畴内，也就是说 512 A100 即可。

**哪些值得注意的实验结果** 作者在大量的任务上进行了测试，其中有几个结果更加值得注意和解读
1. Scale 对算法性能的提升远远没有收敛：一方面是相同规模下的模型，进一步增加 step 数量看上去仍有提升空间，另一方面是进一步提升规模，预计仍然可以提升 downstream task 算法性能。作者也表示：Finally, we plan to release larger models trained on larger pretraining corpora in the future, since we have seen a constant improvement in performance as we were scaling.
2. 训练 loss 的曲线图，可以明显看到更大模型的 loss 曲线更大概率出现更大程度的 loss 异常值，在进一步推进规模时模型训练的稳定性可能会面临更大挑战
3. Natural Question 数据集上，0-shot 显著比之前的算法好（65B vs GPT-3 是 23.8 vs 14.6），这个巨大的区别或许来自训练数据的调整
4. Mathematical reasoning 相关数据集上，Minerva(finetuned by ArXiv and Math Web Pages) 几乎吊打所有 pretrained model。作者并没有解释这里的原因
5. 在 TruthfulQA 上，LLaMA 显著好于 GPT-3，0.57 vs 0.28 of Truthful，0.53 vs 0.25 of Truthful*informative，在作者表示 the rate of correct answers is still low, showing that our model is likely to hallucinate incorrect answers，也就是模型在某些维度上仍然明显不如 GPT-3 上的表现。

**为什么重要？** 模型开源会改变学术界生态，LLM 应用层的研究会爆发，全球更加零散的算力和脑力资源可以投入到这个方向的研究上（而不是超级大组的寡头竞争游戏）。这种学界的变化也会逐步影响到工业界和创业圈子。

**还有哪些问题？** 
1. 详细的分析过程，其中至少包含：1）ablation study 的欠缺，同时 apply 如此多改动，哪些是真的对结果有影响的；2）Error Case Analysis，在具体的 case 上模型呈现出哪些额外的优势或劣势，对这些现象的猜测和归因是什么？
2. Emergent Ability 测试缺失。例如，现有 65B 上有 CoT 能力么？

**ReadMore:** [LLaMA: Open and Efficient Foundation Language Models
](https://scontent.xx.fbcdn.net/v/t39.8562-6/333078981_693988129081760_4712707815225756708_n.pdf?_nc_cat=108&ccb=1-7&_nc_sid=ad8a9d&_nc_ohc=ov6yTHfLfNQAX9g7O_Z&_nc_ht=scontent.xx&oh=00_AfBKmKCfvfjEKu8IDou4FkQnQQ0X8mUYBvi46XpFg90aZw&oe=6403C422)

### 1.2 Text-to-Image 图片生成的 RLHF
将 LLM 中成熟的 RLHF 技术放到图片生成上，提升对 prompt 的可控性。从趋势上来看，图片生成领域的主线是不断提升对图片“变化”的可控性操作。通用的技术框架在这里非常重要。

**如何实现的？**
- 和 LM RLHF 的主要区别
    - 对 human feedback 的表示形式进行了简化，采用了二分类，而不是 ranking，但作者表示“More informative
human feedback, such as ranking, should prove useful when more complex or subjective text prompts are used (e.g., artistic or open-ended generation)”
    - 数据增强，修改 text prompt 使得 text image 不匹配，以此生成大量负例，在 reward model 训练中构建辅助 loss 在这些样本中进行分类
- 数据上使用了 27k image-text ，提升了颜色、数量的背景，尤其是提升了没有见过的维度组合
- rl finetune 训练时，使用了 16k text prompt 训练 reward weighted loss，以及 625k 进行 pre-training loss

**哪些值得注意的实验结果**
- 只用 RLHF 的数据进行训练，会极大损失图片质量，这与 LLM 中 alignment tax 类似。而 loss 中通过 linear combine 来权衡两者似乎不是很对。另外，human feedback 不够丰富也有问题，此项工作的数据都是人工构造的，如果放到生产系统中可能会不太一样

**为什么重要**
1. 对 prompt 干预的角度，方法是通用的；
2. RLHF 在图片生成领域也遇到和 LLM 类似的问题，align 程度和生成质量两者需要 trade off。如何解决 alignment tax 是 RLHF 后续的方向之一
3. 值得思考的问题是人们对于图片修改（或者调整）的干预有哪些形式组成的？从目前的进展来看，更加重要的是给出一个 top-down framework 来修正图片。

**ReadMore** [Aligning Text-to-Image Models using Human Feedback](https://arxiv.org/pdf/2302.12192.pdf)

### 1.3 ViT 22B：最大规模的视觉模型，High Shape Bias！
ViT From 4B To 22B，**“demonstrates the potential for “LLM-like” scaling in vision, and provides key steps towards getting there”**，谷歌的 42 人大型研究团队，改善了 ViT 训练过程中的稳定性和效率，从而提升了 ViT 模型的规模到 22B

**如何实现的？**
1. 网络结构改善，提升硬件效率：将 Attention 和 MLP 并行，由此共享矩阵乘操作
2. MHA 中 QK 结果进行 LayerNorm，显著提升训练稳定性：模型在 8B 规模时开始遇到 loss 不稳定的问题，表现是 attention score 在 softmax 之后会变成接近 one hot 的结果，从而扰乱 loss，由此引入 LayerNorm 解决这个问题。注：这里存在更多细节的推敲，为啥 LayerNorm 可以稳定解决问题？
3. 网络结构超参数，22B 模型显著增加了宽度，而在深度上没有变化，这里作者没有给出更多解释，并且于 LLM 系列工作在这点上完全不同，一方面有可能 Vision 任务就是宽度更加重要，另一方面有可能在扩展深度的时候也遇到优化相关的问题
4. 使用 JFT 数据集（目前已经扩大到 4B 张图片，30k Label），大约训练 3 个 epochs

**有意思的实验结果**作者做了大量的 downstream task 实验，从分类到 dense prediction，均在已有的数据集上将点刷到了新高，不过其中最有信息量的结果是 Human Alignment：High Shape Bias(13% Texture Bias / 87% Shape Bias)，传统的 CNN 网络（如 ResNet）基本上都是 High Texture Bias（70%~80% Texture Bias / 20~30% Shape Bias)，这和人的视觉系统完全不同，为此 CNN 时代还会专门做 extra loss 来加强对 Shape 的捕捉，而 ViT 系列工作，在这点上更加接近人类视觉的体现，从长期而言，这意味着这个系列的工作更具生命力。

**比较遗憾的**是完全缺少了 Power Law 部分的体现，即不知道 22B 是否现有框架已经收敛，也不知道进一步提升规模的瓶颈是什么。

**为什么重要？** Vision Large Model，此项工作解决了 ViT 训练过程中的若干优化问题，规模还会更大。这个过程中通过全面的测试观察算法的表现是最有意思的，例如此项工作中在 Shape Bias 上显著接近人的表现，这点可能比 downstream task 提点更加 impressive，也就是说规模进一步提升后的现象是值得期待的。Scale Matters！

### 1.4 PaLM-E：通用物理机器人的初级形态
谷歌 Robotics 团队结合 *ViT 22B* 和 *PaLM-540B* 两个业界最大模型构成 **PaLM-E 562B**，该模型同时见过 Web Scale 的文本和图片，大模型 + Web Scale 数据，推进了 general grounding。另外，“exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains.” 这种 transfer 能力 demo 出通用物理机器人的初级形态！

**如何实现的？** 简单来说，将图片通过 ViT 22B 转变为 Token Embed，将其于 Text 中 Token Embed 并列输入到 PaLM-540B 中。作者将连续的若干张图（即一个短时的视频片段）和一个句子作为一个样本，预测句子中剩余部分的内容。对机器人场景来说，该模型生成的是用自然语言表达的任务执行步骤（即 Long Horizon Planner In Natural Language），而具体步骤的“自然语言”会有专门的[执](https://ai.googleblog.com/2022/12/talking-to-robots-in-real-time.html)[行器](https://ai.googleblog.com/2022/12/rt-1-robotics-transformer-for-real.html)来操作机器臂完成。

**为什么重要？** 
1. 机器人领域迎来新的突破，通用物理机器人的初级形态，PaLM-E 将复杂任务转变为若干连续的具体任务，再通过自然语言执行器完成机器人的控制。可以试想一下，未来云端的超级 LLM 模型负责复杂任务分解，端上的芯片负责接收具体的指令进行执行。
2. AI 模块之间也会通过自然语言交互进行交互，更本质来看，语言是被证明的通用性的载体（也就是说人类之间的指令交互就是通过语言的方式）从未来的可扩展性来说，这层接口的设计设计为自然语言的确是正确的技术选择。
3. “PaLM-E is trained on a mixture of diverse tasks across multiple robot embodiments as well as general vision-language tasks. Importantly, we have demonstrated that this diverse training leads to several avenues of transfer from the vision language domains into embodied decision making, enabling robot planning tasks to be achieved data efficiently.”
4. 从多模态的角度来看，ground to 物理世界是 LLM 超车的机会。
5. 更加说明 Foundation Model 的重要性，会作为未来各个 AI 系统的必要组成模块。