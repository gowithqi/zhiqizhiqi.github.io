---
title: "Week-AI-02：Pytorch2.0/GPT-4/USM/Vid2Seq/ART/LERF"
date: 2023-03-20T11:30:03+00:00
# weight: 1
# aliases: ["/first"]
tags: ["Week"]
# author: ["Me", "You"] # multiple authors

draft: true

description: "todo"
# canonicalURL: "https://canonical.url/to/page"
disableHLJS: true # to disable highlightjs
disableShare: false
disableHLJS: false
hideSummary: false
searchHidden: true
---


## 0. TakeAway & 思考

## 1. 前沿技术追踪
### 1.1 Pytorch 2.0：全球免费获赠 1 倍算力！
引入 torch.compile 这个大坑（包含了若干底层技术），DL 开发者以很低的开发成本享受计算效率（榨干硬件）的提升，可以预见未来很长一段时间，整个技术栈仍然会快速发展。另外，针对 transformer 的优化也是值得注意的，尤其是 MHA pytorch2.0 实现，包含了 FlashAttention 和 xFormers 等第三方的实现，开发者可以通过 pytorch 2.0 接口直接支持各种高度优化的 MHA 实现

**torch.compile**
1. 效果：163 个开源模型 93% 可以直接调用 torch.compile，**在 A100 上快 43%；FP32 精度下，快 21%，混合精度下，快 51%** ---> 免费的软件午餐，今年应该不需要买新机器了吧
2. "From day one, we knew the performance limits of eager execution" and "Our key criteria was to preserve certain kinds of flexibility – support for dynamic shapes and dynamic programs which researchers use in various stages of exploration."
3. Part-1 Graph Acquisition：TorchDynamo + AOTAutograd，放弃了若干不合适的尝试（比如"TorchScript, FX tracing, Lazy Tensors."），这些尝试很难在 flexible / user-experience / fast 上都靠谱
    1. TorchDynamo vs TorchScript，在 7000+ Github Pytorch project 中前者可以捕捉到 99% ，而后者只有 50%（而且有很大的 overhead）。TorchDynamo 在 python bytecode 这层进行 hook，获取全部的计算图，也就是说理论上不止适用 pytorch 接口，所有 python 调用都可以进行优化。ReadMore: [TorchDynamo](https://pytorch.org/docs/stable/dynamo/index.html)
    2. AOTAutograd 用来捕捉 bp 计算图，复用了 torch_dispatch ，可以 "capture the backwards pass “ahead-of-time”."
4. Part-2 Graph Lowering：Aten/Prim IR
5. Part-3 Graph Compilation：
    1. TorchInductor enable by Triton，自动将 pytorch model 编译为 GPU 上的 triton 代码或者 cpu 上的 c++/openmp 代码；TorchInductor 核心 IR 只包含 [50 个 op](https://github.com/pytorch/pytorch/blob/master/torch/_inductor/codegen/triton.py)，未来还会进一步扩展；
    2. PrimTorch 定义 250 个左右的 low-level op，适合编译器进行 fuse together 以获得更好的性能

**为什么重要**
1. 免费向全球供给将近 1 倍 GPU 算力，某种程度来说对整个学术界的进展是巨大的推动
2. 平台型软件可以不断供给算力，layer by layer 的分层解耦会不断增加 overhead 吞掉硬件性能；很容易预测的是 compile 的引入使得 pytorch 未来性能提升仍有较大的空间
3. Triton 吃掉 Cuda：以 block 作为编程抽象，相对于 SIMT(single instruct multi-thread) 更加适合 deep learning，开发者在 python 端就可以写出和 hand-written 性能相当的 NN 算子实现。
4. 额外的问题是 pytorch 之上的中间件应该如何设计接口呢？例如 ColossalAI/deepspeed 等 

